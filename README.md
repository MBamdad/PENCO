# PENCO: Physicsâ€“Energyâ€“Numericalâ€“Consistent Operator for 3D Phase-Field Modeling

This repository contains the official implementation of **PENCO** (Physicsâ€“Energyâ€“Numericalâ€“Consistent Operator), a hybrid neural operator framework for **three-dimensional phase-field PDEs**.

PENCO combines:
- **Neural operators** (spectral/Fourier-based),
- **Physics constraints** (PDE residuals evaluated at symmetric Gaussâ€“Lobatto collocation points),
- **Numerical consistency** (alignment with a stable semi-implicit update),
- **Thermodynamic energy dissipation** (enforcing one-sided free-energy decay),

to achieve **accurate, stable, and physically consistent** long-horizon predictions with **very limited training data**.

---

## 1. Problem Setting

We consider phase-field-type PDEs of the form

> âˆ‚â‚œu = ð’©(u, âˆ‡u, âˆ‡Â²u, â€¦),

where:
- **u(x, t)** is a scalar order parameter on a periodic 3D domain Î© âŠ‚ â„Â³,
- ð’© is a nonlinear differential operator (e.g., Allenâ€“Cahn, Cahnâ€“Hilliard, etc.).

The goal is to learn a **solution operator**

> ð’¢\_Î¸ : a(x) â†¦ u(x, t),

that maps an initial condition **a(x)** to the full spatio-temporal trajectory **u(x, t)**.

Supervision is provided by a small number of **high-fidelity spectral simulations** (semi-implicit Fourier-spectral solver) for several benchmark PDEs:
- Allenâ€“Cahn (AC),
- Cahnâ€“Hilliard (CH),
- Swiftâ€“Hohenberg (SH),
- Phase-Field Crystal (PFC),
- Molecular Beam Epitaxy (MBE).

---

## 2. Baseline Neural Operators

We benchmark PENCO against two state-of-the-art neural operator architectures.

### 2.1 Four-Dimensional Fourier Neural Operator (FNO-4D)

- Learns mappings between function spaces using **spectral convolutions** in space.
- Time is treated as an additional input channel; Fourier transform is applied only in spatial dimensions (x, y, z).
- Architecture structure:
  - Lifting network ð’«(a(x, t)) â†’ latent representation vâ‚€(x, t),
  - Sequence of spectral layers combining pointwise linear maps and Fourier convolutions,
  - Projection network ð’¬ that maps latent features back to physical space.
- Predicts blocks of time steps in a **single forward pass**, avoiding explicit recurrence.

**Limitations:**
- No explicit causal structure over time,
- Temporal errors accumulate over long horizons,
- No explicit enforcement of physics or numerical structure.

### 2.2 Multi-Head Neural Operator (MHNO)

- Uses a **shared spectral encoder** for spatial features and **time-step-specific heads** for temporal evolution.
- Components:
  - Spectral encoder: shared across all times, maps the initial field to a high-dimensional latent representation.
  - Projection networks ð’¬â‚™: time-dependent decoders for each time level.
  - Transition networks â„‹â‚™: propagate information between successive time steps.

**Advantages:**
- Captures long-range temporal dependencies,
- Improves temporal coherence over purely parallel or purely autoregressive designs.

**Limitations:**
- Still trained with a purely data-driven loss (mean-squared error),
- No explicit guarantee of energy decay or numerical scheme consistency.

---

## 3. PENCO: Physicsâ€“Energyâ€“Numericalâ€“Consistent Operator

PENCO keeps a compact **spectral neural operator backbone** but fundamentally changes the **training objective**.

Instead of only minimizing a data regression loss, PENCO optimizes

> ð“›\_total = (1 âˆ’ Î») Â· Î± Â· ð“›\_data + Î» Â· ð“›\_phys,  with Î» âˆˆ [0, 1],

where:
- **ð“›_data**: standard prediction error against the ground-truth solution, measured using the MSE formulation.
- **ð“›_phys**: physics-guided regularization composed of:
  - **PDE residuals** at symmetric Gaussâ€“Lobatto collocation points,
  - **Numerical scheme consistency** via a semi-implicit reference update,
  - **Energy dissipation** through one-sided free-energy decay enforcement,
  - **Low-frequency spectral anchoring** to stabilize large-scale modes.

A typical choice:
- Î± = 10Â³,
- Î» = 0.25 for hybrid training,
- Î» = 1.0 for pure-physics training.

### ð“›_colloc (PDE collocation residual)

Enforces the PDE inside each time step using LÂ² Gaussâ€“Lobatto collocation.  
The residual is evaluated at two temporal points:

**Ï„â‚,â‚‚ = 1/2 Â± 1/(2âˆš5)**

At each Ï„, the PDE residual is computed as:

- the predicted time derivative  
  **(uÌ‚â¿âºÂ¹ âˆ’ uâ¿) / Î”t**
- minus the PDE right-hand side evaluated at the interpolated state  
  **(1 âˆ’ Ï„)Â·uâ¿ + Ï„Â·uÌ‚â¿âºÂ¹**
  
The two residuals are normalized and combined through their LÂ² norm to form the collocation loss.


- **ð“›\_scheme (numerical scheme consistency)**  
  Aligns the network update with a **semi-implicit IMEX time-stepping scheme**:
  - Linear stiff terms treated implicitly,
  - Nonlinear terms treated explicitly,
  - Mobility operator encodes gradient flow geometry.  
  The neural prediction is penalized if it deviates from the semi-implicit update.

- **ð“›\_energy (thermodynamic consistency)**  
  Penalizes any **increase in free energy** between successive time steps:
  - E[uâ¿âºÂ¹] â‰¤ E[uâ¿] enforced via a one-sided penalty,
  - E[u] includes double-well bulk energy and gradient regularization terms (and higher-order contributions when relevant).

- **ð“›\_anchor (low-frequency spectral anchoring)**  
  Compares the **low-wavenumber Fourier modes** of:
  - predicted field, and
  - semi-implicit teacher.  
  This prevents large-scale spectral drift and maintains coherent long-range structure.

### 3.2 Time-Dependent Weights

- **wâ‚ = 10â»Â³** and **wâ‚ƒ = 0.3** (fixed throughout training).

- **wâ‚‚ and wâ‚„ (epoch-dependent weights)**  
  These weights evolve smoothly over training following:

  ```
  wâ‚‚(e) = 0.32 âˆ’ 0.12 * (e / (E âˆ’ 1))
  ```

  ```
  wâ‚„(e) = 0.25 + 0.6 * (e / (E âˆ’ 1))Â²
  ```

  where **e** is the current epoch and **E** is the total number of epochs.

  - **Early training:** larger wâ‚‚ provides stronger emphasis on scheme consistency, improving short-horizon stability.  
  - **Later training:** increasing wâ‚„ strengthens spectral anchoring, reducing long-horizon drift and stabilizing low-frequency modes.

---

## 4. Data Generation and Training Protocol

### 4.1 Data Generation

- Training trajectories: **N âˆˆ {50, 100, 200}** per PDE.
- Spatial discretization: 3D periodic cube using a **Fourier-spectral** method.
- Time integration: **semi-implicit IMEX** scheme (the same scheme that defines the numerical teacher).
- Initial conditions:
  - Gaussian Random Fields (GRFs) with randomized statistical parameters and thresholds,
  - This yields a wide range of interface geometries, characteristic length scales, and volume fractions.

The numerical simulations and training are implemented in **PyTorch** and run on NVIDIA GPUs, ensuring consistent floating-point behavior between solver and neural operator.

### 4.2 Training Budget

To isolate **data efficiency** from pure computational budget:

- The **test set size is fixed** (50 trajectories per PDE).
- The **number of gradient updates** is kept constant for a given PDE:
  - Larger datasets do *not* receive more optimizer steps.
  - In the pure-physics limit (Î» = 1), the effective number of updates is fixed, independent of N.

The operator architecture is intentionally compact:
- 2 spectral layers,
- Moderate latent width,
- ~800k trainable parameters,
- Spatial resolution: 32Â³,
- Temporal horizon: Nâ‚œ = 100 time steps.

---

## 5. Numerical Experiments

All models (FNO-4D, MHNO, PENCO hybrid, PENCO pure-physics) are evaluated on the unified phase-field system

> âˆ‚â‚œu = Î±â‚[âˆ‡Â²u âˆ’ (1/ÎµÂ²) fâ€²(u)]  
>       + Î±â‚‚ âˆ‡Â²[âˆ’ÎµÂ² âˆ‡Â²u + fâ€²(u)]  
>       + Î±â‚ƒ[âˆ’uÂ³ âˆ’ (1 âˆ’ Îµ)u + 2âˆ‡Â²u âˆ’ âˆ‡â´u]  
>       + Î±â‚„ âˆ‡Â²[uÂ³ + (1 âˆ’ Îµ)u + 2âˆ‡Â²u + âˆ‡â´u]  
>       + Î±â‚…[âˆ’Îµ âˆ‡â´u âˆ’ âˆ‡Â·((1 âˆ’ |âˆ‡u|Â²) âˆ‡u)],

with coefficients Î±â‚,â€¦,Î±â‚… selecting AC, CH, SH, PFC, or MBE.

Performance is measured via a normalized LÂ² error over spaceâ€“time:

> Error =  
>  sqrt( Î£\_i |Ã›(xáµ¢, táµ¢) âˆ’ u(xáµ¢, táµ¢)|Â² ) / sqrt( Î£\_i |u(xáµ¢, táµ¢)|Â² ),

where Ã› is the model prediction and u is the reference solution.

### 5.1 In-Distribution Performance (GRF Initial Conditions)

Example: 3D **Allenâ€“Cahn** equation:

- The phase field evolves under competition between:
  - a double-well reaction term driving u toward Â±1, and
  - a diffusion term that regularizes interfaces.

**Observations:**

- **FNO-4D**:
  - Tracks early-time dynamics,
  - Gradually oversmooths,
  - Small structures disappear,
  - Topology degrades at later times.

- **MHNO**:
  - Better temporal coherence due to time-step-specific heads,
  - Still exhibits noticeable drift on long horizons.

- **PENCO (Î» = 0.25)**:
  - Preserves interface sharpness and correct coarsening rate,
  - Maintains morphology consistent with the reference until final time,
  - Strongly suppresses long-horizon error accumulation.

- **Pure-physics (Î» = 1)**:
  - Stable and qualitatively correct,
  - Slightly less accurate in fine details compared to hybrid PENCO.


## ðŸ”¬ Numerical Examples

Below we present representative 2D and 3D phase-evolution results for each PDE system using the PENCO hybrid operator.  
These animations demonstrate interface sharpening, coarsening behavior, and long-horizon stability achieved by physicsâ€“numerical consistent training.

---

## **Allenâ€“Cahn (AC)**

### **2D Phase Evolution**
The 2D contour evolution highlights PENCOâ€™s ability to maintain sharp interfaces, correct curvature-driven smoothing, and consistent coarsening over the full simulation horizon.

![](https://github.com/MBamdad/PENCO/blob/main/AC3D_Hybrid/hybrid_ac3d/AC3d_results/AC3D_2D_contour_evolution.gif)

### **3D Phase Evolution**
The 3D iso-surface evolution captures full volumetric coarsening behavior. PENCO preserves morphology, interface topology, and large-scale structure more accurately than purely data-driven baselines.

<!-- Replace this with your actual 3D GIF path -->
![](https://github.com/MBamdad/PENCO/blob/main/AC3D_Hybrid/hybrid_ac3d/AC3d_results/AC3D_isosurface_evolution.gif)

---

## **Cahnâ€“Hilliard (CH)**

### **2D Phase Evolution**
<!-- Replace this link with the CH 2D GIF file -->
![](https://github.com/MBamdad/PENCO/blob/main/AC3D_Hybrid/hybrid_ac3d/CH3d_results/CH3D_2D_contour_evolution.gif)

### **3D Phase Evolution**
<!-- Replace this link with CH 3D GIF file -->
![](https://github.com/MBamdad/PENCO/blob/main/AC3D_Hybrid/hybrid_ac3d/CH3d_results/CH3D_isosurface_evolution.gif)

---

## **Swiftâ€“Hohenberg (SH)**

### **2D Evolution**
<!-- Replace with SH 2D -->
![](https://github.com/MBamdad/PENCO/blob/main/AC3D_Hybrid/hybrid_ac3d/SH3D_resluts/SH3D_2D_contour_evolution.gif)

### **3D Evolution**
<!-- Replace with SH 3D -->
![](https://github.com/MBamdad/PENCO/blob/main/AC3D_Hybrid/hybrid_ac3d/SH3D_resluts/SH3D_isosurface_evolution.gif)

---

## **Phase-Field Crystal (PFC)**

### **2D Evolution**
<!-- Replace with PFC 2D -->
![](https://github.com/MBamdad/PENCO/blob/main/AC3D_Hybrid/hybrid_ac3d/PFC3D_results/PFC3D_2D_contour_evolution.gif)

### **3D Evolution**
<!-- Replace with PFC 3D -->
![](https://github.com/MBamdad/PENCO/blob/main/AC3D_Hybrid/hybrid_ac3d/PFC3D_results/PFC3D_isosurface_evolution.gif)

---

## **Molecular Beam Epitaxy (MBE)**

### **2D Evolution**
<!-- Replace with MBE 2D -->
![](https://github.com/MBamdad/PENCO/blob/main/AC3D_Hybrid/hybrid_ac3d/MBE3d_results/MBE3D_2D_contour_evolution.gif)

### **3D Evolution**
<!-- Replace with MBE 3D -->
![](https://github.com/MBamdad/PENCO/blob/main/AC3D_Hybrid/hybrid_ac3d/MBE3d_results/MBE3D_isosurface_evolution.gif)

---


### 5.2 ðŸ“˜ Out-of-Distribution (OOD) Results

To test robustness beyond GRF-based initial conditions, we evaluate on **deterministic geometries**:

- **AC, CH, SH**: smooth spheres
- **PFC**: anisotropic star-shaped interface
- **MBE**: torus configuration

These shapes differ significantly from training data in curvature, interface width, and topology, making them a stringent test of OOD generalization.


**Summary of OOD behavior:**

- **FNO-4D**:
  - Highest error in almost all OOD settings,
  - Error grows rapidly as dynamics depart from GRF statistics.

- **MHNO**:
  - Consistently better than FNO-4D,
  - Captures sharper interfaces and non-GRF structures more faithfully.

- **PENCO (Hybrid)**:
  - Best overall OOD performance across most systems (especially SH, PFC, MBE),
  - Physics-based regularization removes overshoot and stabilizes long-horizon rollouts,
  - Maintains coherent evolution even for highly non-GRF geometries.

- **Pure-Physics**:
  - Particularly strong for CH, SH, and MBE, where conservative structure or curvature-driven coarsening is dominant,
  - Slightly outperforms PENCO in CH due to strict mass conservation encoded in the PDE,
  - Generally less detailed than the hybrid model.

A complete collection of all OOD evaluation plots (spherical, star-shaped, torus initializations across all PDEs) is provided in the PDF below:
![](https://github.com/MBamdad/PENCO/blob/main/AC3D_Hybrid/hybrid_ac3d/OOD_plots-1.png)

---

## 6. Key Takeaways

Compared to FNO-4D and MHNO, PENCO demonstrates:

- **Superior long-horizon stability and accuracy**, enabled by physics-consistent training,
- **High data efficiency**, achieving strong performance with only 50â€“200 training trajectories,
- **Robust OOD generalization** to geometries entirely absent from the training distribution (spherical, star-shaped, torus initializations),
- **Physically faithful evolution**, consistently preserving energy decay, correct coarsening rates, interface sharpness, and wavelength-selection behavior.

Furthermore, PENCO achieves these improvements with a much lighter computational footprint, including:

- **32Â³ spatial resolution**, far smaller than typical operator-learning configurations,
- **Only two spectral operator layers**, minimizing model depth while retaining expressiveness,
- **A training horizon reduced by a factor of ten**, achieving stable convergence with only ~10% of the epochs normally required,
- **No loss in accuracy, stability, or generalization**, despite the significantly reduced computational budget.

Together, these factors demonstrate PENCOâ€™s efficiency and scalability for large-scale 3D phase-field modeling.

---

## 7. Citation

If you would like others to cite this work, you can add a BibTeX entry here once the paper is on arXiv or published, for example:

```bibtex
@article{bamdad2025penco,
  title   = {PENCO: A Physicsâ€“Energyâ€“Numericalâ€“Consistent Operator for 3D Phase Field Modeling},
  author  = {Bamdad, Mostafa and Eshaghi, Mohammad Sadegh and Anitescu, Cosmin and Valizadeh, Navid and Rabczuk, Timon},
  journal = arXiv,
  year    = {2025},
}
```
